# K2A Enterprise Monitoring Values
# Based on kagent patterns and enterprise requirements

# Global configuration
global:
  # Image registry configuration
  imageRegistry: quay.io/k2a-enterprise
  imagePullPolicy: IfNotPresent
  imagePullSecrets: []

  # Common labels applied to all resources
  commonLabels:
    app.kubernetes.io/part-of: k2a-monitoring
    app.kubernetes.io/managed-by: Helm

  # OpenShift specific settings
  openshift:
    enabled: true
    createSCC: true
    route:
      enabled: true
      tls:
        termination: edge
        insecureEdgeTerminationPolicy: Redirect

# K2A CRDs subchart
k2a-crds:
  enabled: true

# K2A Tools subchart
k2a-tools:
  enabled: true
  grafana:
    enabled: true
  prometheus:
    enabled: true

# Controller configuration
controller:
  enabled: true

  image:
    repository: k2a-monitoring-controller
    tag: v1.0.0
    pullPolicy: IfNotPresent

  # Replica configuration
  replicaCount: 2

  # Resource configuration
  resources:
    limits:
      cpu: 500m
      memory: 512Mi
    requests:
      cpu: 100m
      memory: 128Mi

  # Security context
  securityContext:
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL

  # Pod security context
  podSecurityContext:
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    seccompProfile:
      type: RuntimeDefault

  # Environment variables
  env:
    LOG_LEVEL: info
    METRICS_PORT: "8080"
    HEALTH_PORT: "8081"
    LEADER_ELECTION: "true"

  # Service configuration
  service:
    type: ClusterIP
    ports:
      metrics: 8080
      health: 8081
      webhook: 9443

  # Health checks
  livenessProbe:
    httpGet:
      path: /healthz
      port: health
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 3

  readinessProbe:
    httpGet:
      path: /readyz
      port: health
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 3

  # Affinity and tolerations
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
        - weight: 100
          podAffinityTerm:
            labelSelector:
              matchExpressions:
                - key: app.kubernetes.io/name
                  operator: In
                  values:
                    - k2a-monitoring-controller
            topologyKey: kubernetes.io/hostname

  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
      effect: NoSchedule
    - key: node-role.kubernetes.io/control-plane
      operator: Exists
      effect: NoSchedule

  # Volume mounts
  volumeMounts:
    - name: tmp
      mountPath: /tmp
    - name: cache
      mountPath: /var/cache/k2a

  volumes:
    - name: tmp
      emptyDir: {}
    - name: cache
      emptyDir: {}

# Agent configuration
agent:
  enabled: true

  image:
    repository: k2a-monitoring-agent
    tag: v1.0.0
    pullPolicy: IfNotPresent

  # Replica configuration based on environment
  replicaCount: 2

  # Auto-scaling configuration
  autoscaling:
    enabled: false
    minReplicas: 2
    maxReplicas: 10
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 80

  # Pod Disruption Budget
  podDisruptionBudget:
    enabled: false
    minAvailable: 1

  # Resource configuration
  resources:
    limits:
      cpu: 1000m
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi

  # Security context (inherited from controller)
  securityContext:
    runAsNonRoot: true
    runAsUser: 1001
    runAsGroup: 1001
    fsGroup: 1001
    allowPrivilegeEscalation: false
    readOnlyRootFilesystem: true
    capabilities:
      drop:
        - ALL

  # Monitoring configuration
  monitoring:
    interval: 30s
    timeout: 10s
    metrics:
      nodes: true
      pods: true
      services: true
      persistentVolumes: false
    namespaces: []  # Empty = all namespaces

  # Alert thresholds
  alerting:
    enabled: true
    thresholds:
      cpu: 80
      memory: 85
      disk: 90
      podRestarts: 5

  # Export configuration
  export:
    enabled: false
    formats: ["prometheus", "json"]
    s3:
      enabled: false
      bucket: ""
      region: ""
      credentialsSecret: ""

  # Service configuration
  service:
    type: ClusterIP
    ports:
      http: 8080
      metrics: 8081
      health: 8082
    annotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "8081"
      prometheus.io/path: "/metrics"

# RBAC configuration
rbac:
  enabled: true

  # Service account
  serviceAccount:
    create: true
    annotations: {}
    name: ""
    automountServiceAccountToken: true

  # Cluster role configuration
  clusterRole:
    create: true
    name: ""
    rules:
      # Core Kubernetes resources
      - apiGroups: [""]
        resources:
          - nodes
          - nodes/metrics
          - nodes/stats
          - services
          - endpoints
          - pods
          - pods/log
          - pods/status
          - configmaps
          - secrets
          - persistentvolumes
          - persistentvolumeclaims
          - namespaces
          - events
        verbs: ["get", "list", "watch"]

      # Apps resources
      - apiGroups: ["apps"]
        resources:
          - deployments
          - daemonsets
          - replicasets
          - statefulsets
        verbs: ["get", "list", "watch"]

      # Metrics
      - apiGroups: ["metrics.k8s.io"]
        resources: ["nodes", "pods"]
        verbs: ["get", "list"]

      # OpenShift specific
      - apiGroups: ["config.openshift.io"]
        resources:
          - clusterversions
          - infrastructures
          - networks
        verbs: ["get", "list", "watch"]

      - apiGroups: ["route.openshift.io"]
        resources: ["routes"]
        verbs: ["get", "list", "watch"]

      # Monitoring
      - apiGroups: ["monitoring.coreos.com"]
        resources:
          - servicemonitors
          - prometheusrules
        verbs: ["get", "list", "watch", "create", "update", "patch"]

# Security Context Constraints (OpenShift)
scc:
  enabled: true
  name: k2a-monitoring-scc
  allowHostDirVolumePlugin: false
  allowHostIPC: false
  allowHostNetwork: false
  allowHostPID: false
  allowHostPorts: false
  allowPrivilegedContainer: false
  allowedCapabilities: []
  defaultAddCapabilities: []
  requiredDropCapabilities: ["ALL"]
  readOnlyRootFilesystem: true
  runAsUser:
    type: MustRunAsNonRoot
  seLinuxContext:
    type: MustRunAs
  fsGroup:
    type: MustRunAs
    ranges:
      - min: 1000
        max: 65535
  supplementalGroups:
    type: MustRunAs
    ranges:
      - min: 1000
        max: 65535

# Network Policies
networkPolicy:
  enabled: true
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: openshift-monitoring
      ports:
        - protocol: TCP
          port: 8080
        - protocol: TCP
          port: 8081
  egress:
    - to: []
      ports:
        - protocol: TCP
          port: 443
        - protocol: TCP
          port: 6443
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53

# Monitoring and Observability
monitoring:
  enabled: true

  # ServiceMonitor for Prometheus
  serviceMonitor:
    enabled: true
    namespace: ""
    labels: {}
    interval: 30s
    path: /metrics
    scheme: http
    relabelings: []
    metricRelabelings: []

  # PrometheusRule for alerts
  prometheusRule:
    enabled: true
    namespace: ""
    labels: {}
    rules:
      - alert: K2AAgentDown
        expr: up{job="k2a-monitoring"} == 0
        for: 2m
        labels:
          severity: critical
          service: k2a-monitoring
        annotations:
          summary: "K2A Monitoring Agent is down"
          description: "K2A Monitoring Agent has been down for more than 2 minutes"

      - alert: K2AHighMemoryUsage
        expr: k2a_agent_memory_usage_bytes / k2a_agent_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: k2a-monitoring
        annotations:
          summary: "K2A Agent high memory usage"
          description: "K2A Agent memory usage is above 90%: {{ $value }}%"

# ConfigMap for agent configuration
config:
  create: true
  data:
    config.yaml: |
      server:
        port: 8080
        metrics_port: 8081
        health_port: 8082
      monitoring:
        interval: 30s
        timeout: 10s
      cluster:
        name: "{{ .Values.global.clusterName | default "openshift-cluster" }}"
        environment: "{{ .Values.global.environment | default "production" }}"
      collection:
        nodes:
          enabled: {{ .Values.agent.monitoring.metrics.nodes }}
        pods:
          enabled: {{ .Values.agent.monitoring.metrics.pods }}
        services:
          enabled: {{ .Values.agent.monitoring.metrics.services }}
      alerting:
        enabled: {{ .Values.agent.alerting.enabled }}
        thresholds:
          node_cpu_high: {{ .Values.agent.alerting.thresholds.cpu }}
          node_memory_high: {{ .Values.agent.alerting.thresholds.memory }}
          pod_restart_high: {{ .Values.agent.alerting.thresholds.podRestarts }}
      logging:
        level: "{{ .Values.controller.env.LOG_LEVEL | default "info" }}"
        format: json

# Environment-specific overrides
environments:
  dev:
    controller:
      replicaCount: 1
      resources:
        limits:
          cpu: 200m
          memory: 256Mi
        requests:
          cpu: 50m
          memory: 64Mi
    agent:
      replicaCount: 1
      resources:
        limits:
          cpu: 500m
          memory: 512Mi
        requests:
          cpu: 100m
          memory: 128Mi

  staging:
    controller:
      replicaCount: 2
    agent:
      replicaCount: 2
      autoscaling:
        enabled: false

  prod:
    controller:
      replicaCount: 3
    agent:
      replicaCount: 3
      autoscaling:
        enabled: true
        minReplicas: 3
        maxReplicas: 10
      podDisruptionBudget:
        enabled: true
        minAvailable: 2